{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c793d1c",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323e27d",
   "metadata": {},
   "source": [
    "## Copy CSV files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95d59a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assume all the pre-requisites were set up \n",
    "%store -r setup_instance_check_passed\n",
    "%store -r setup_dependencies_passed\n",
    "%store -r setup_s3_bucket_passed\n",
    "%store -r setup_iam_roles_passed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27abfe19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a390ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Public Data\n",
    "!aws s3 ls s3://ads508team5/\n",
    "\n",
    "# Copy Datasets from :\n",
    "# Source of data\n",
    "s3_public_path_tweeter = \"s3://ads508team5/tweeter\"\n",
    "s3_public_path_nyt = \"s3://ads508team5/nyt\"\n",
    "s3_public_path_cities = \"s3://ads508team5/cities\"\n",
    "\n",
    "# Destination Data:\n",
    "s3_private_path_tweeter = \"s3://{}/ADS508_project/tweeter\".format(bucket)\n",
    "s3_private_path_nyt = \"s3://{}/ADS508_project/nyt\".format(bucket)\n",
    "s3_private_path_cities = \"s3://{}/ADS508_project/cities\".format(bucket)\n",
    "\n",
    "# Copy datasets\n",
    "!aws s3 cp --recursive $s3_public_path_tweeter/ $s3_private_path_tweeter/ --exclude \"*\" --include \"hashtag_donaldtrump.csv\"\n",
    "!aws s3 cp --recursive $s3_public_path_tweeter/ $s3_private_path_tweeter/ --exclude \"*\" --include \"hashtag_joebiden.csv\"\n",
    "!aws s3 cp --recursive $s3_public_path_nyt/ $s3_private_path_nyt/ --exclude \"*\" --include \"nyt-comments-2020.csv\"\n",
    "!aws s3 cp --recursive $s3_public_path_cities/ $s3_private_path_cities/ --exclude \"*\" --include \"uscities.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd7b2f",
   "metadata": {},
   "source": [
    "## Create Database, Tables and Parquets\n",
    "### Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed3ea97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup and check pre-requisites to create Database\n",
    "ingest_create_athena_db_passed = False\n",
    "\n",
    "!pip install --disable-pip-version-check -q PyAthena==2.1.0\n",
    "from pyathena import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba41e8-462e-425b-a617-05641521dd0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set S3 staging directory -- this is a temporary directory used for Athena queries\n",
    "s3_staging_dir = \"s3://{0}/athena/staging\".format(bucket)\n",
    "\n",
    "# Create Connection\n",
    "conn = connect(region_name=region, s3_staging_dir=s3_staging_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6268521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Database\n",
    "database_name = \"dbpoliticpulsecomment\"\n",
    "\n",
    "statement = \"CREATE DATABASE IF NOT EXISTS {}\".format(database_name)\n",
    "\n",
    "pd.read_sql(statement, conn)\n",
    "\n",
    "# Verify DB successfully created\n",
    "statement = \"SHOW DATABASES\"\n",
    "\n",
    "df_show = pd.read_sql(statement, conn)\n",
    "df_show.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58edb18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# End of Create Database\n",
    "if database_name in df_show.values:\n",
    "    ingest_create_athena_db_passed = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6701e",
   "metadata": {},
   "source": [
    "### Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366b2af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Tweeter Tables\n",
    "table_name_csv = \"tweeter\"\n",
    "s3_private_path_tweeter = \"s3://{}/ADS508_project/tweeter\".format(bucket)\n",
    "\n",
    "statement = \"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS {}.{}(\n",
    "  created_at TIMESTAMP,\n",
    "  tweet_id FLOAT,\n",
    "  tweet VARCHAR(250),\n",
    "  likes INT,\n",
    "  retweet_count INT,\n",
    "  source VARCHAR(45),\n",
    "  user_id INT,\n",
    "  user_name VARCHAR(250),\n",
    "  user_screen_name VARCHAR(45),\n",
    "  user_description VARCHAR(250),\n",
    "  user_join_date TIMESTAMP,\n",
    "  user_followers_count INT,\n",
    "  user_location VARCHAR(45),\n",
    "  lat FLOAT,\n",
    "  long FLOAT,\n",
    "  city VARCHAR(45),\n",
    "  country VARCHAR(45),\n",
    "  continent VARCHAR(45),\n",
    "  state VARCHAR(45),\n",
    "  state_code VARCHAR(45),\n",
    "  collected_at VARCHAR(45)\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LOCATION '{}'\n",
    "TBLPROPERTIES ('skip.header.line.count'='1')\"\"\".format(\n",
    "    database_name, table_name_csv, s3_private_path_tweeter\n",
    ")\n",
    "\n",
    "pd.read_sql(statement, conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f3816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing hashtag_donaldtrump.csv tweet\n",
    "tweet = \"You get a tie! And you get a tie! #Trump ‘s rally #Iowa https://t.co/jJalUUmh5D\"\n",
    "# testing hashtag_joebiden.csv tweet\n",
    "tweet = \"@chrislongview Watching and setting dvr. Let’s give him bonus ratings!! #JoeBiden\"\n",
    "\n",
    "\n",
    "statement = \"\"\"SELECT * FROM {}.{}\n",
    "    WHERE tweet = '{}' LIMIT 100\"\"\".format(\n",
    "    database_name, table_name_csv, tweet\n",
    ")\n",
    "\n",
    "df = pd.read_sql(statement, conn)\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed105b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create NYT comment Table\n",
    "table_name_csv = \"nyt_comment\"\n",
    "s3_private_path_nyt = \"s3://{}/ADS508_project/nyt\".format(bucket)\n",
    "\n",
    "\n",
    "statement = \"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS {}.{} (\n",
    "  commentID INT,\n",
    "  status VARCHAR(20),\n",
    "  commentSequence INT,\n",
    "  userID INT,\n",
    "  userDisplayName VARCHAR(45),\n",
    "  userLocation VARCHAR(45),\n",
    "  userTitle VARCHAR(10),\n",
    "  commentBody VARCHAR(500),\n",
    "  createDate TIMESTAMP,\n",
    "  updateDate TIMESTAMP,\n",
    "  approveDate TIMESTAMP,\n",
    "  recommendation INT,\n",
    "  replyCount INT,\n",
    "  editorsSelection VARCHAR(20),\n",
    "  parentID INT,\n",
    "  parentUserDisplayName VARCHAR(45),\n",
    "  depth INT,\n",
    "  commentType VARCHAR(20),\n",
    "  trusted VARCHAR(20),\n",
    "  recommendedFlag VARCHAR(20),\n",
    "  permID INT,\n",
    "  isAnonymous VARCHAR(20),\n",
    "  articleID VARCHAR(150)\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LOCATION '{}'\n",
    "TBLPROPERTIES ('skip.header.line.count'='1')\"\"\".format(\n",
    "    database_name, table_name_csv, s3_private_path_nyt\n",
    ")\n",
    "\n",
    "pd.read_sql(statement, conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08d61f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statement = \"SHOW TABLES in {}\".format(database_name)\n",
    "\n",
    "df_show = pd.read_sql(statement, conn)\n",
    "df_show.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f118e12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "commentBody = \"@Philip Brown Agree 110%.\"\n",
    "\n",
    "statement = \"\"\"SELECT * FROM {}.{}\n",
    "    WHERE commentBody = '{}' LIMIT 100\"\"\".format(\n",
    "    database_name, table_name_csv, commentBody\n",
    ")\n",
    "\n",
    "df = pd.read_sql(statement, conn)\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df721c6c-eb56-473c-991f-4a8a80486d15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create uscities table\n",
    "table_name = \"uscities\"\n",
    "s3_private_path_cities = \"s3://{}/ADS508_project/cities\".format(bucket)\n",
    "\n",
    "statement = \"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {}.{} (\n",
    "    city VARCHAR(45),\n",
    "    state_id VARCHAR(2),\n",
    "    state_name VARCHAR(30)\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LOCATION '{}'\n",
    "TBLPROPERTIES ('skip.header.line.count'='1')\n",
    "\"\"\".format(\n",
    "    database_name, table_name, s3_private_path_cities\n",
    ")\n",
    "\n",
    "pd.read_sql(statement, conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ccd002",
   "metadata": {},
   "source": [
    "### Parquets\n",
    "#### Feature Selection and Creation\n",
    "Prior creating a parquet table, tweeter and NYT comment tables are going to be combined under Comment Table in Athena where a feature creation is going to be performed. When the table is combined, we selected some features that would be benefit to our goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878fc4ed-1442-4639-b29b-a1ec495856cd",
   "metadata": {},
   "source": [
    "Selected features which could help on train the model are comment_body, user_location, candidatepoll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcc6e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_comment = \"comment\"\n",
    "table_tweeter = \"tweeter\"\n",
    "table_nyt_comment = \"nyt_comment\"\n",
    "\n",
    "# SQL Statement combine tweeter and NYT comment in the preparation to create parquet\n",
    "statement = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {}.{} AS (\n",
    "SELECT DISTINCT\n",
    "    tweet_id AS comment_id,\n",
    "    tweet AS comment_body,\n",
    "    user_id AS user_id,\n",
    "    user_name AS user_name,\n",
    "    user_location AS user_location,\n",
    "    u.state_id AS user_state,\n",
    "    created_at AS create_date,\n",
    "    retweet_count AS reply_retweet,\n",
    "    likes AS recommendation_like,\n",
    "    'tweet' as source,\n",
    "    (LENGTH(tweet) - LENGTH(REPLACE(LOWER(tweet), 'trump', ''))) / LENGTH('trump') AS trump_count,\n",
    "    (LENGTH(tweet) - LENGTH(REPLACE(LOWER(tweet), 'biden', ''))) / LENGTH('biden') AS biden_count,\n",
    "    CASE \n",
    "        WHEN \n",
    "            (LENGTH(tweet) - LENGTH(REPLACE(LOWER(tweet), 'biden', ''))) / LENGTH('biden') > \n",
    "            (LENGTH(tweet) - LENGTH(REPLACE(LOWER(tweet), 'trump', ''))) / LENGTH('trump') \n",
    "        THEN 'Biden'\n",
    "        WHEN              \n",
    "            (LENGTH(tweet) - LENGTH(REPLACE(LOWER(tweet), 'biden', ''))) / LENGTH('biden') < \n",
    "            (LENGTH(tweet) - LENGTH(REPLACE(LOWER(tweet), 'trump', ''))) / LENGTH('trump') \n",
    "        THEN 'Trump'\n",
    "        ELSE NULL\n",
    "    END AS candidatepoll\n",
    "FROM\n",
    "    {}.{}\n",
    "LEFT JOIN\n",
    "    dbpoliticpulsecomment.uscities u ON lower(user_location) = lower(u.city) OR \n",
    "    lower(user_location) = lower(u.state_name) OR \n",
    "    lower(user_location) = lower(u.state_id)\n",
    "WHERE\n",
    "    LENGTH(tweet) > 3\n",
    "UNION\n",
    "SELECT DISTINCT\n",
    "    commentid AS comment_id,\n",
    "    commentbody AS comment_body,\n",
    "    userID AS user_id,\n",
    "    userDisplayName AS user_name,\n",
    "    userLocation AS user_location,\n",
    "    u.state_id as user_state,\n",
    "    createDate AS create_date,\n",
    "    replyCount AS reply_retweet,\n",
    "    recommendation AS recommendation_like,\n",
    "    'nyt_comment' as source,\n",
    "    (LENGTH(commentbody) - LENGTH(REPLACE(LOWER(commentbody), 'trump', ''))) / LENGTH('trump') AS trump_count,\n",
    "    (LENGTH(commentbody) - LENGTH(REPLACE(LOWER(commentbody), 'biden', ''))) / LENGTH('biden') AS biden_count,\n",
    "    CASE\n",
    "        WHEN\n",
    "            (LENGTH(commentbody) - LENGTH(REPLACE(LOWER(commentbody), 'biden', ''))) / LENGTH('biden') > \n",
    "            (LENGTH(commentbody) - LENGTH(REPLACE(LOWER(commentbody), 'trump', ''))) / LENGTH('trump') \n",
    "        THEN 'Biden'\n",
    "        WHEN\n",
    "            (LENGTH(commentbody) - LENGTH(REPLACE(LOWER(commentbody), 'biden', ''))) / LENGTH('biden') < \n",
    "            (LENGTH(commentbody) - LENGTH(REPLACE(LOWER(commentbody), 'trump', ''))) / LENGTH('trump') \n",
    "        THEN 'Trump'\n",
    "        ELSE NULL\n",
    "    END AS candidatepoll\n",
    "FROM \n",
    "    {}.{}\n",
    "LEFT JOIN\n",
    "    dbpoliticpulsecomment.uscities u ON lower(userLocation) = lower(u.city) OR \n",
    "    lower(userLocation) = lower(u.state_name) OR \n",
    "    lower(userLocation) = lower(u.state_id)\n",
    "WHERE LENGTH(commentbody) > 3\n",
    ")\"\"\".format(database_name, table_comment,database_name, table_tweeter,database_name, table_nyt_comment)\n",
    "\n",
    "pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf2ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "commentbody = \"Joe Biden\"\n",
    "source = \"nyt_comment\"\n",
    "\n",
    "statement = \"\"\"SELECT * FROM {}.{}\n",
    "    WHERE comment_body like '{}%' and source = '{}' LIMIT 100\"\"\".format(\n",
    "    database_name, table_comment, commentbody, source\n",
    ")\n",
    "\n",
    "df = pd.read_sql(statement, conn)\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e266db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup to create Parquet\n",
    "ingest_create_athena_table_parquet_passed = False\n",
    "\n",
    "# Set S3 path to Parquet data\n",
    "s3_path_parquet = \"s3://{}/ADS508_project/parquet\".format(bucket)\n",
    "\n",
    "table_parquet = \"comment_parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae06efcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SQL statement to execute (remove comment_id, user_name,)\n",
    "statement = \"\"\"CREATE TABLE IF NOT EXISTS {}.{}\n",
    "WITH (format = 'PARQUET', external_location = '{}', partitioned_by = ARRAY['candidatepoll']) AS\n",
    "SELECT DISTINCT user_location,\n",
    "         user_state,\n",
    "         comment_body,\n",
    "         source,\n",
    "         candidatepoll\n",
    "FROM {}.{}\n",
    "where candidatepoll is not null AND Length(comment_body)>1 AND length(user_location)>1\"\"\".format(\n",
    "    database_name, table_parquet, s3_path_parquet, database_name, table_comment\n",
    ")\n",
    "pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435eaa44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load the Parquet partitions\n",
    "statement = \"MSCK REPAIR TABLE {}.{}\".format(database_name, table_parquet)\n",
    "\n",
    "df = pd.read_sql(statement, conn)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e21991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statement = \"SHOW PARTITIONS {}.{}\".format(database_name, table_parquet)\n",
    "df_partitions = pd.read_sql(statement, conn)\n",
    "df_partitions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282d08a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statement = \"SHOW TABLES in {}\".format(database_name)\n",
    "df_partitions = pd.read_sql(statement, conn)\n",
    "df_partitions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc60922",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "candidatepoll = \"Trump\"\n",
    "\n",
    "statement = \"\"\"SELECT * FROM {}.{}\n",
    "    Where candidatepoll = '{}' LIMIT 10\"\"\".format(\n",
    "    database_name, table_parquet, candidatepoll\n",
    ")\n",
    "df_parquet = pd.read_sql(statement, conn)\n",
    "df_parquet.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8461c5",
   "metadata": {},
   "source": [
    "## Query Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e00bda0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup and check pre-requisites to create Database\n",
    "\n",
    "!pip install --disable-pip-version-check -q awswrangler\n",
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28f75b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_filter = lambda x: x[\"candidatepoll\"] == \"Trump\"\n",
    "b_filter = lambda x: x[\"candidatepoll\"] == \"Biden\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208219a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path =  \"s3://{}/ADS508_project/parquet\".format(bucket)\n",
    "\n",
    "df_parquet_results = wr.s3.read_parquet(\n",
    "    path, columns=[\"comment_body\", \"source\", \"user_location\", \"candidatepoll\"], partition_filter=t_filter, dataset=True\n",
    ")\n",
    "\n",
    "#\"user_state\", \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40daa73f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_parquet_results_trump = df_parquet_results.drop_duplicates(subset = ['comment_body'])\n",
    "df_parquet_results_trump.shape\n",
    "df_parquet_results_trump.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076d4959",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path =  \"s3://{}/ADS508_project/parquet\".format(bucket)\n",
    "\n",
    "# Call all public discourse data from \n",
    "df_parquet_results = wr.s3.read_parquet(\n",
    "    path, columns=[\"comment_body\", \"source\", \"user_location\", \"candidatepoll\"], partition_filter=b_filter, dataset=True\n",
    ")\n",
    "\n",
    "# \"user_state\",\n",
    "df_parquet_results_biden = df_parquet_results.drop_duplicates(subset = ['comment_body'])\n",
    "df_parquet_results_biden.shape\n",
    "df_parquet_results_biden.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a2655",
   "metadata": {},
   "source": [
    "# Data Exploration before Any Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5accb614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking for missing values from trump data\n",
    "df_parquet_results_trump.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af933a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for missing values from  biden data\n",
    "df_parquet_results_biden.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23647e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Distributions for Trump and Biden \n",
    "df_parquet_results_trump['source'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82b8ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Distributions for Trump and Biden \n",
    "df_parquet_results_biden['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d75cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filtering Data by Location \n",
    "df_parquet_results_trump['user_location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f505d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Top 30 locations of social media activity\n",
    "df_parquet_results_trump['user_location'].value_counts().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3724bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_parquet_results_biden['user_location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138776c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Top 30 locations of social media activity\n",
    "df_parquet_results_biden['user_location'].value_counts().head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14395879",
   "metadata": {},
   "source": [
    "# Create EDA Overview Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f766c5a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Sources for Each Candidate\n",
    "# Code from ChatGPT\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "candidate = ['Trump','Biden']\n",
    "twitter_n = [df_parquet_results_trump['source'].value_counts()[0],df_parquet_results_biden['source'].value_counts()[0]]\n",
    "nyt_n = [df_parquet_results_trump['source'].value_counts()[1],df_parquet_results_biden['source'].value_counts()[1]]\n",
    "\n",
    "x = range(len(candidate))\n",
    "\n",
    "# Plotting the bars\n",
    "plt.bar(x, twitter_n, width=0.4, label='Twitter Mentions', align='center')\n",
    "plt.bar(x, nyt_n, width=0.4, label='NYT Mentions', align='edge')\n",
    "\n",
    "# Adding labels\n",
    "plt.xlabel('Candidates')\n",
    "plt.ylabel('Number of Mentions')\n",
    "plt.title('Twitter and NYT Mentions by Candidate')\n",
    "plt.xticks(x, candidate)\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.savefig('DataDist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7632eaa-5bd6-4c9a-9d97-343f65725f1d",
   "metadata": {},
   "source": [
    "## Standardizing Text Entries and Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe9814-bf24-4c15-aef0-6b356d759407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1d1b97-7d7f-4e69-9e39-6d0014cd81e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ffe246-eea5-488d-aa14-a91557970a62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "# Function to preprocess text data\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Replace emojis with text labels\n",
    "    text = emoji.demojize(text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    # Remove \"#\" from hashtags\n",
    "    text = re.sub(r'#', '', text)\n",
    "    # Remove \"@\" from mentions\n",
    "    text = re.sub(r'@', '', text)\n",
    "    # Remove extra spaces while preserving at least one space between words\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Check if the text consists only of numbers\n",
    "    if text.isdigit():\n",
    "        text = \"NA\"\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d70e6d-5659-4631-a6ff-60f784382f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_parquet_results_biden['user_location'] = df_parquet_results_biden['user_location'].astype(str)\n",
    "df_parquet_results_trump['user_location'] = df_parquet_results_trump['user_location'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778319e7-81a4-4ac8-bc8f-422439ed78a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing to text columns in the dataframe\n",
    "df_parquet_results_biden['clean_text'] = df_parquet_results_biden['comment_body'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767cb35-9241-4867-a89e-58d591f9843b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_parquet_results_biden['clean_location'] = df_parquet_results_biden['user_location'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa3eb2-40b6-4a0e-b2d4-f94ab7e07518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_parquet_results_trump['clean_text'] = df_parquet_results_trump['comment_body'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb3bcd-26f5-4f63-ae3f-a46bb458676c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_parquet_results_trump['clean_location'] = df_parquet_results_trump['user_location'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e1423-e301-4802-9ff6-02cd48c5786a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_parquet_results_trump.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306e619-0424-4eb9-bcf7-7074732e4e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_parquet_results_biden.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1175b11-6feb-497a-bdae-9aadd86b1216",
   "metadata": {},
   "source": [
    "# Word Clouds with Comment Body Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167ad62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install WordCloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Sample a subset of comments for generating the word cloud\n",
    "sampled_trump_comments = df_parquet_results_trump['clean_text'].dropna().sample(n=10000, random_state=42)\n",
    "\n",
    "# Concatenate sampled comments for Trump\n",
    "trump_comments_text = ' '.join(sampled_trump_comments)\n",
    "\n",
    "# Define words and patterns to exclude\n",
    "words_to_exclude = ['trump', 'donald', 'donaldtrump', 'https', 'co', 'st'] \n",
    "patterns_to_exclude = [r'\\btrump\\b', r'\\bdonald\\b', r'\\bdonaldtrump\\b', r'https?://\\S+']\n",
    "\n",
    "# Combine words and patterns to exclude\n",
    "exclude_patterns = '|'.join(words_to_exclude + patterns_to_exclude)\n",
    "\n",
    "# Preprocess text to remove specific words and patterns\n",
    "trump_comments_text_cleaned = re.sub(exclude_patterns, '', trump_comments_text, flags=re.IGNORECASE)\n",
    "\n",
    "# Generate word cloud for Trump comments with a limit on the number of words\n",
    "wordcloud_trump = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(trump_comments_text_cleaned)\n",
    "\n",
    "# Plot the word cloud for Trump\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud_trump, interpolation='bilinear')\n",
    "plt.title('Word Cloud for Trump Comments (Excluding \"Trump\" and \"Donald\")')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ae0744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample a subset of comments for Biden\n",
    "sampled_biden_comments = df_parquet_results_biden['clean_text'].dropna().sample(n=40000, random_state=42)\n",
    "\n",
    "# Concatenate sampled comments for Biden\n",
    "biden_comments_text = ' '.join(sampled_biden_comments)\n",
    "\n",
    "# Define words and patterns to exclude\n",
    "words_to_exclude = ['biden', 'joe', 'joebiden', 'president', 'election', 'https', 'co','amp']  # Exclude common words and URLs\n",
    "patterns_to_exclude = [r'\\bbiden\\b', r'\\bjoe\\b', r'\\bjoebiden\\b', r'https?://\\S+']\n",
    "\n",
    "# Combine words and patterns to exclude\n",
    "exclude_patterns = '|'.join(words_to_exclude + patterns_to_exclude)\n",
    "\n",
    "# Preprocess text to remove specific words and patterns\n",
    "biden_comments_text_cleaned = re.sub(exclude_patterns, '', biden_comments_text, flags=re.IGNORECASE)\n",
    "\n",
    "# Generate word cloud for Biden comments with a limit on the number of words\n",
    "wordcloud_biden = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(biden_comments_text_cleaned)\n",
    "\n",
    "# Plot the word cloud for Biden\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud_biden, interpolation='bilinear')\n",
    "plt.title('Word Cloud for Biden Comments (Excluding \"Biden\" and \"Joe\")')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c3a9ea-b173-4900-8692-e7c1f1155e6e",
   "metadata": {},
   "source": [
    "# Feature Transformation and Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c2e43",
   "metadata": {},
   "source": [
    "### Sentiment Labels Using Textblob and Stripping Out Non-English Text Entries with Fasttext."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a8dcc-df72-421d-9fc3-3659e469f28d",
   "metadata": {},
   "source": [
    "### This code must be ran on Mac/Linux only. Windows is not compatible. Clean data from this code is stored in an S3 bucket and called below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c46a727-7977-4640-a92d-47a14077e776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39bb947-1078-444b-b9c0-7f678b55fa2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install fasttext-wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629b6913-e034-45b5-bd6b-5591b53fb1f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Use pretrained model\n",
    "#!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin -P /ADS508_GroupProject\n",
    "#!ls -p /ADS508_GroupProject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f114ed-2edd-4f25-9e22-7ce19c1e9fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410a09f4-2936-43c6-8a60-86f847f8440e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import fasttext\n",
    "# Load the pre-trained language identification model\n",
    "#model_path = '/ADS508_GroupProject/lid.176.bin'  # Path to the pre-trained language identification model\n",
    "#model = fasttext.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d240da-ee48-4eab-8050-fb5683b4b575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to detect language using FastText\n",
    "#def detect_language_fasttext(text):\n",
    "#    prediction = model.predict(text)\n",
    "#    language_code = prediction[0][0].split('__label__')[1]\n",
    "#    return language_code\n",
    "\n",
    "# Function to analyze tweet sentiment to catch non english words\n",
    "#def analyze_tweet_sentiment(tweet_text):\n",
    "#    try:\n",
    "#        language = detect_language_fasttext(tweet_text)\n",
    "#        if language == 'en':\n",
    "#            analysis = TextBlob(tweet_text)\n",
    "#            return analysis.sentiment.polarity\n",
    "#        else:\n",
    "#            return None\n",
    "#    except:\n",
    "#        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c08286a-166b-4167-ac5c-93c1173cfc50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install textblob\n",
    "from textblob import TextBlob\n",
    "def analyze_tweet_sentiment(tweet_text):\n",
    "    analysis = TextBlob(tweet_text)\n",
    "    # Return sentiment polarity (ranging from -1 to 1)\n",
    "    return analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5faf7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply sentiment labels to Trump Data\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming you have a DataFrame named df_tweets with a column 'text' containing tweet text\n",
    "    df_tweets_trump = df_parquet_results_trump[['clean_text']]\n",
    "    # Analyze sentiment for each tweet in the DataFrame\n",
    "    df_tweets_trump['sentiment_score'] = df_tweets_trump['clean_text'].apply(analyze_tweet_sentiment)\n",
    "\n",
    "# Add sentiment output to parent dataframe\n",
    "df_parquet_results_trump['sentiment'] = df_tweets_trump['sentiment_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ce372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # Apply Sentiment labels to Biden Data       \n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming you have a DataFrame named df_tweets with a column 'text' containing tweet text\n",
    "    df_tweets_biden = df_parquet_results_biden[['clean_text']]\n",
    "    # Analyze sentiment for each tweet in the DataFrame\n",
    "    df_tweets_biden['sentiment_score'] = df_tweets_biden['clean_text'].apply(analyze_tweet_sentiment)\n",
    "\n",
    "# Add sentiment output to parent dataframe\n",
    "df_parquet_results_biden['sentiment'] = df_tweets_biden['sentiment_score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f32e3-4a20-4ebf-af29-3118266a6e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_parquet_results_trump.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd892e56-316f-4da2-aaea-5d37972e390d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_parquet_results_biden.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f675ba-cb4c-4681-a8ed-4095613c30c2",
   "metadata": {},
   "source": [
    "## Combine candidate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823a470b-897c-4ee4-8c73-91797cd5dd95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_parquet_results_biden, df_parquet_results_trump], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3bed61-bf06-4962-a3d9-b2ee380a869b",
   "metadata": {},
   "source": [
    "### Create sentiment labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac638a3-e39f-464a-9d5d-e62610c83c20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined['sentiment_category'] = df_combined['sentiment'].apply(lambda x: 'negative' if x < 0 else ('positive' if x > 0 else 'neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1b4b6-22ee-4ea8-ac8b-d09370bd2643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df_combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80dc4e-6650-4296-a0b8-4d3f6de3d721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577fd5f2-190d-4d58-af7c-5cbd2e9e6283",
   "metadata": {},
   "source": [
    "# Additional EDA with Clean Language Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f8f6a5-f04b-440e-92e2-e74251399309",
   "metadata": {},
   "source": [
    "## Trump Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6888f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Examine Trump Sentiment distribution\n",
    "df_combined[df_combined['candidatepoll'] == 'Trump']['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f86222-5614-403e-9196-6721f3af0c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined[(df_combined['candidatepoll'] == 'Trump') & (df_combined['sentiment'] < 0)].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6147c1f4-5e6a-4d13-ae45-65b611fa39f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined[(df_combined['candidatepoll'] == 'Trump') & (df_combined['sentiment'] == 0)].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a426e1-a2b3-4d05-acc1-08998daafbd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined[(df_combined['candidatepoll'] == 'Trump') & (df_combined['sentiment'] > 0)].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99336874-a3d8-48e2-9ded-7ccbdd43742a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(df_combined[df_combined['candidatepoll'] == 'Trump']['sentiment'], bins=30, edgecolor='black')\n",
    "\n",
    "# Adding labels\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentiment Scores for Trump')\n",
    "\n",
    "# Show plot\n",
    "plt.savefig('TrumpSentDist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299af5c5-43e8-4ce6-bd79-da2b8655e8fa",
   "metadata": {},
   "source": [
    "### Biden Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e76cf-ee32-4e92-b30d-e8a8b93a02c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined[df_combined['candidatepoll'] == 'Biden']['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244e68a-0598-4e64-9045-935a278343d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined[(df_combined['candidatepoll'] == 'Biden') & (df_combined['sentiment'] < 0)].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f9bd3-dd1c-4f2a-adb1-c14b1fffbf1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined[(df_combined['candidatepoll'] == 'Biden') & (df_combined['sentiment'] == 0)].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a688224a-92ab-4847-a543-2aa62a4a5d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined[(df_combined['candidatepoll'] == 'Biden') & (df_combined['sentiment'] > 0)].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc073d8-90f7-41b1-a35a-c9808fefe863",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(df_combined[df_combined['candidatepoll'] == 'Biden']['sentiment'], bins=30, edgecolor='black')\n",
    "\n",
    "# Adding labels\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentiment Scores for Biden')\n",
    "\n",
    "# Show plot\n",
    "plt.savefig('BidenSentDist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd63b6a-94d5-4cd2-b0bd-b264ce307a9d",
   "metadata": {},
   "source": [
    "## Data Source Proportions with Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc59399-dbf4-4568-9134-7c1541267549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "candidate = ['Trump','Biden']\n",
    "twitter_n = [df_combined[df_combined['candidatepoll'] == 'Trump']['source'].value_counts()[0], \n",
    "             df_combined[df_combined['candidatepoll'] == 'Biden']['source'].value_counts()[0]]\n",
    "nyt_n = [df_combined[df_combined['candidatepoll'] == 'Trump']['source'].value_counts()[1], \n",
    "         df_combined[df_combined['candidatepoll'] == 'Biden']['source'].value_counts()[1]]\n",
    "\n",
    "x = range(len(candidate))\n",
    "\n",
    "# Plotting the bars\n",
    "plt.bar(x, twitter_n, width=0.4, label='Twitter Mentions', align='center')\n",
    "plt.bar(x, nyt_n, width=0.4, label='NYT Mentions', align='edge')\n",
    "\n",
    "# Adding labels\n",
    "plt.xlabel('Candidates')\n",
    "plt.ylabel('Number of Mentions')\n",
    "plt.title('Twitter and NYT Mentions by Candidate')\n",
    "plt.xticks(x, candidate)\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.savefig('DataDist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed86825-ea70-4071-a61b-d33d32377e10",
   "metadata": {},
   "source": [
    "# Additional Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089e0065-555e-423a-b885-1f1a201b4790",
   "metadata": {},
   "source": [
    "## Create Outcome Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82621bac-f971-49d0-8414-9b9a4beef5c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined['SentimentOutcome'] = df_combined['candidatepoll']+\"_\"+df_combined['sentiment_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db276d11-b4de-4d84-9ffa-691f2d31d622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "category_counts = df_combined['SentimentOutcome'].value_counts()\n",
    "plt.bar(category_counts.index, category_counts.values)\n",
    "plt.xlabel('Candidate Sentiment Categories')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels by 45 degrees\n",
    "plt.ylabel('Count')\n",
    "plt.title('Frequency of Each Possible Sentiment Outcome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbaa582-21c3-4a70-b0bf-13b2d72c1c07",
   "metadata": {},
   "source": [
    "## Smallest category is Biden_negative. When using train/test/split with 90/5/5 ratio, we will need balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da0cb8-cac5-41a3-9bc6-9281e8cd4f53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " df_combined[df_combined['candidatepoll'] == 'Biden']['SentimentOutcome'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75960b0-12b4-4fe3-8bef-ab345510ed2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " df_combined[df_combined['candidatepoll'] == 'Trump']['SentimentOutcome'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4881c3-1007-4683-bff2-49bad3cd4873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9382803-5b54-4bec-a507-42d58cc4492f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "292aae2a-b305-4315-ba4c-72ff14256dfd",
   "metadata": {},
   "source": [
    "# Save Data to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888eb3b-afa5-4a99-9c30-ecdc342517da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the S3 bucket name\n",
    "prefix = \"ADS508_project/cleandata/\"\n",
    "\n",
    "# List of corresponding file names\n",
    "file_name = \"cleandata.csv\"\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "#refine df_combined\n",
    "df_combined_ref = df_combined[['source', 'clean_text', 'clean_location', 'candidatepoll', 'SentimentOutcome']]\n",
    "df_combined_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d6d42-96cd-484e-be60-df59c4aef981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload each file to S3\n",
    "df_combined_ref.to_csv(file_name, index=False)\n",
    "s3.upload_file(file_name, bucket, prefix + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f969c20e-7615-49e2-80cf-c51dddcaeffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create temp table of df_combined\n",
    "temp_table = \"temp_table\"\n",
    "s3_private_path_cleandata = \"s3://{}/ADS508_project/cleandata\".format(bucket)\n",
    "\n",
    "statement = \"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {}.{} (\n",
    "    source VARCHAR(20),\n",
    "    clean_text VARCHAR(250),\n",
    "    clean_location VARCHAR(25),\n",
    "    candidatepoll VARCHAR(10),\n",
    "    SentimentOutcome VARCHAR(10)\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LOCATION '{}'\n",
    "TBLPROPERTIES ('skip.header.line.count'='1')\n",
    "\"\"\".format(\n",
    "    database_name, temp_table, s3_private_path_cleandata\n",
    ")\n",
    "\n",
    "pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887cc49-19b4-43ee-bfb5-5780d3f997ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# map df_combined/ cleandata with uscities by city\n",
    "temp_table = \"temp_table_city\"\n",
    "import time\n",
    "from datetime import datetime\n",
    "from time import strftime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "print(f\"start: {timestamp}\\n\")\n",
    "\n",
    "statement = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {}.{} AS (\n",
    "SELECT DISTINCT \n",
    "    t.source, \n",
    "    t.clean_text, \n",
    "    t.clean_location, \n",
    "    t.candidatepoll, \n",
    "    t.SentimentOutcome, \n",
    "    u.state_id\n",
    "    FROM  dbpoliticpulsecomment.temp_table t\n",
    "    LEFT JOIN dbpoliticpulsecomment.uscities u\n",
    "        ON lower(t.clean_location) = lower(u.city)\n",
    "    where state_id <> ''\n",
    ")\n",
    "\"\"\".format(database_name, temp_table)\n",
    "pd.read_sql(statement, conn)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "print(f\"end: {timestamp}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca32b5-2eb6-4ee4-9436-564471413d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# map df_combined/ cleandata with uscities by statename\n",
    "temp_table = \"temp_table_statename\"\n",
    "import time\n",
    "from datetime import datetime\n",
    "from time import strftime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "print(f\"start: {timestamp}\\n\")\n",
    "\n",
    "statement = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {}.{} AS (\n",
    "SELECT DISTINCT \n",
    "    t.source, \n",
    "    t.clean_text, \n",
    "    t.clean_location, \n",
    "    t.candidatepoll, \n",
    "    t.SentimentOutcome, \n",
    "    u.state_id\n",
    "    FROM  dbpoliticpulsecomment.temp_table t\n",
    "    LEFT JOIN dbpoliticpulsecomment.uscities u\n",
    "        ON lower(t.clean_location) = lower(u.state_name)\n",
    "    where state_id <> ''\n",
    ")\n",
    "\"\"\".format(database_name, temp_table)\n",
    "pd.read_sql(statement, conn)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "print(f\"end: {timestamp}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9366df17-d9ec-43b2-96cf-77fa14457e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# map df_combined/ cleandata with uscities by stateid\n",
    "temp_table = \"temp_table_stateid\"\n",
    "import time\n",
    "from datetime import datetime\n",
    "from time import strftime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "print(f\"start: {timestamp}\\n\")\n",
    "\n",
    "statement = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {}.{} AS (\n",
    "SELECT DISTINCT \n",
    "    t.source, \n",
    "    t.clean_text, \n",
    "    t.clean_location, \n",
    "    t.candidatepoll, \n",
    "    t.SentimentOutcome, \n",
    "    u.state_id\n",
    "    FROM  dbpoliticpulsecomment.temp_table t\n",
    "    LEFT JOIN dbpoliticpulsecomment.uscities u\n",
    "        ON lower(t.clean_location) = lower(u.state_id)\n",
    "    where state_id <> ''\n",
    ")\n",
    "\"\"\".format(database_name, temp_table)\n",
    "pd.read_sql(statement, conn)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "print(f\"end: {timestamp}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb53375-29bb-45e1-aa6d-9a5de7cbc444",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a combine of temp_table_state_id, _statename, _city\n",
    "table_name = \"cleandata\"\n",
    "import time\n",
    "from datetime import datetime\n",
    "from time import strftime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "print(f\"start: {timestamp}\\n\")\n",
    "\n",
    "statement = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {}.{} AS (\n",
    "SELECT DISTINCT \n",
    "    source, \n",
    "    clean_text, \n",
    "    clean_location, \n",
    "    candidatepoll, \n",
    "    SentimentOutcome, \n",
    "    state_id\n",
    "    FROM  dbpoliticpulsecomment.temp_table_stateid\n",
    "UNION\n",
    "SELECT DISTINCT \n",
    "    source, \n",
    "    clean_text, \n",
    "    clean_location, \n",
    "    candidatepoll, \n",
    "    SentimentOutcome, \n",
    "    state_id\n",
    "    FROM  dbpoliticpulsecomment.temp_table_city\n",
    "UNION\n",
    "SELECT DISTINCT \n",
    "    source, \n",
    "    clean_text, \n",
    "    clean_location, \n",
    "    candidatepoll, \n",
    "    SentimentOutcome, \n",
    "    state_id\n",
    "    FROM  dbpoliticpulsecomment.temp_table_statename\n",
    ")\n",
    "\"\"\".format(database_name, table_name)\n",
    "pd.read_sql(statement, conn)\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "print(f\"end: {timestamp}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f8e0c-7861-44f7-b8a6-83e7245484ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop all unused tables from Athena\n",
    "drop_table_names = [\"temp_table\", \"temp_table_statename\", \"temp_table_stateid\", \"temp_table_city\"]\n",
    "\n",
    "for table_name in drop_table_names:\n",
    "    table_name = table_name\n",
    "\n",
    "    statement = \"\"\"\n",
    "    DROP TABLE {}.{}\n",
    "    \"\"\".format(database_name, table_name)\n",
    "    \n",
    "    pd.read_sql(statement, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d8ed8-db2f-4ac0-a48f-e0c4c8b3e827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#==> SHOULD CREATE ANOTHER PARQUET OR READING FROM THIS? \n",
    "table_name = \"cleandata\"\n",
    "statement = \"\"\"SELECT * FROM {}.{}\"\"\".format(\n",
    "    database_name, table_name\n",
    ")\n",
    "df_combined_clean = pd.read_sql(statement, conn)\n",
    "df_combined_clean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01d3f5-67b0-4464-9a89-5592907eb9c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98258a41-a25c-46eb-b0a2-90bf9df379b8",
   "metadata": {},
   "source": [
    "# Examine State IDs by Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539929ea-ea45-4e0f-ac6c-9cedc27d746c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final = df_combined_clean[['clean_text', 'state_id', 'sentimentoutcome']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d7e14-cd85-4d5b-9a5a-1379b2bae87f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final['state_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d53367b-6e65-4987-8cc7-8d5f9e674d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Filter the DataFrame for the top 10 states\n",
    "top_5_states = df_final['state_id'].value_counts().head(5).index\n",
    "filtered_df = df_final[df_final['state_id'].isin(top_5_states)]\n",
    "\n",
    "# Step 2: Create a cross-tabulation (crosstab) with 'state_id' and 'sentimentoutcome'\n",
    "cross_tab = pd.crosstab(filtered_df['state_id'], filtered_df['sentimentoutcome'])\n",
    "\n",
    "# Display the cross-tabulation\n",
    "print(cross_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11048e6-07b2-473b-b05b-119613469fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f729516d-f171-46ca-a061-b9f6ba020efd",
   "metadata": {},
   "source": [
    "# Upload Final Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0492cc0a-9011-444e-81d3-38ccd197b556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the S3 bucket name\n",
    "prefix = \"ADS508_project/cleandata/\"\n",
    "\n",
    "# List of corresponding file names\n",
    "file_name = \"final_data.csv\"\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "df_final.to_csv(file_name, index=False)\n",
    "s3.upload_file(file_name, bucket, prefix + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236b793-9419-4e37-8b79-70284e4718b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b47e0c10-4188-4cfa-87ed-ddad6a5cdb59",
   "metadata": {},
   "source": [
    "---> USING PYTHON AND TOOK FOREVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b032b1-8cbe-4c6a-9575-90c61c612c39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read csv US City from S3 ---> PLANNING TO MOVE IT IN .py AND USE PREPROCESSING JOB FROM SAGEMAKER\n",
    "#git_city_loc = \"https://raw.githubusercontent.com/HNStaggs/ADS508_GroupProject/main/uscities.csv\"\n",
    "#df_city = pd.read_csv(git_city_loc)\n",
    "\n",
    "#df_parquet_results_trump['clean_location'].head(15000)\n",
    "#df_parquet_results_trump1 = df_parquet_results_trump.head(100)\n",
    "#df_parquet_results_trump1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6a12d-f3ee-4e6c-9eb4-900447fbb259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PREFER TO RUN IT IN .py with SageMaker processing job\n",
    "# TRY IT WITH 10000 RECORD took FOREVER\n",
    "\n",
    "#import time\n",
    "#from datetime import datetime\n",
    "#from time import strftime\n",
    "#timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "#print(f\"start: {timestamp}\\n\")\n",
    "\n",
    "# Create master location table\n",
    "#git_city_loc = \"https://raw.githubusercontent.com/HNStaggs/ADS508_GroupProject/main/uscities.csv\"\n",
    "#df_city = pd.read_csv(git_city_loc)\n",
    "#master_location = df_city\n",
    "\n",
    "# Function to map city and state name to state ID\n",
    "#def map_state_id(city):\n",
    "#    for index, loc in master_location.iterrows():\n",
    "#        if city.lower() == loc['city'].lower() or city.lower() == loc['state_name'].lower():\n",
    "#            return loc['state_id']\n",
    "#    return None\n",
    "\n",
    "# Apply mapping function to user location table\n",
    "#df_parquet_results_trump1['final_location'] = df_parquet_results_trump1['clean_location'].apply(map_state_id)  # Use clean location data from primary cleaning step\n",
    "\n",
    "# Display the user location table with State ID column added\n",
    "#timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "#print(f\"end: {timestamp}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eefe85-292d-47ac-a8fa-ff66688e34f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_parquet_results_trump1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1567296d-4225-426c-8b00-7994c19b5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "### working progress.. still need to check##\n",
    "#from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "# Create a ScriptProcessor object\n",
    "#script_processor = ScriptProcessor(base_job_name='data-processing',\n",
    "#                                   image_uri=None,\n",
    "#                                   command=['python3'],\n",
    "#                                   instance_count=1,\n",
    "#                                   instance_type='ml.m5.xlarge',\n",
    "#                                   role=role)\n",
    "\n",
    "# Specify the entry script\n",
    "#entry_script = 's3://sagemaker-us-west-2-471112815505/ADS508_project/script/process_loc.py'\n",
    "\n",
    "# Run the processing job\n",
    "#script_processor.run(code=entry_script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b83ffc-a653-46fb-893d-cf0be75bfafc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Vis\n",
    "# # creat colored maps by state based on sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c08574e-b16b-49ed-ae5b-ba8093d2b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determime sample size from each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6914a545-71c8-4aee-9660-0ce7ad36e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine portion of Trump/Biden for each state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51555eeb-fa23-4cf0-af8c-9f3759e36a3a",
   "metadata": {},
   "source": [
    "# Load Final Data from S3 Bucket for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee90830-3b4f-4608-b5a0-2dad7d9b6973",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>state_id</th>\n",
       "      <th>sentimentoutcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H A Hyde Agreed I wonder how many trumpers kn...</td>\n",
       "      <td>NY</td>\n",
       "      <td>Trump_posi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FunkyIrishman I would love that but if SCOTUS...</td>\n",
       "      <td>LA</td>\n",
       "      <td>Trump_posi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember when Trump said What have you got to...</td>\n",
       "      <td>CT</td>\n",
       "      <td>Trump_neut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Seems to me that current events mean that Tru...</td>\n",
       "      <td>CA</td>\n",
       "      <td>Trump_nega</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One of the reasons we have Donald Trump in off...</td>\n",
       "      <td>NJ</td>\n",
       "      <td>Trump_posi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text state_id sentimentoutcome\n",
       "0   H A Hyde Agreed I wonder how many trumpers kn...       NY       Trump_posi\n",
       "1   FunkyIrishman I would love that but if SCOTUS...       LA       Trump_posi\n",
       "2   Remember when Trump said What have you got to...       CT       Trump_neut\n",
       "3   Seems to me that current events mean that Tru...       CA       Trump_nega\n",
       "4  One of the reasons we have Donald Trump in off...       NJ       Trump_posi"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the S3 bucket name and file path\n",
    "bucket_name = bucket\n",
    "file_path = 'ADS508_project/cleandata/final_data.csv'\n",
    "\n",
    "# Read CSV file from S3 bucket into DataFrame\n",
    "df_final = pd.read_csv(f's3://{bucket_name}/{file_path}')\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a4865af-17d4-4add-a91c-2dbbdcf3e36f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330248, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bddbfcd-fc4e-4de7-a49a-d2f86f6d6742",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Autopilot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a315da-70a7-47c4-b886-45c0bf832273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f9f288-d742-4422-99b9-58f6946ebbfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Sample about 10% of the data without replacement ---just to test------\n",
    "# df_sampled = df_final.sample(frac=0.1, random_state=42)\n",
    "# # Save the sampled DataFrame to a CSV file, without the index\n",
    "# df_sampled.to_csv(\"df_sampled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b538ccc-83d7-4883-a3aa-fa356114e24d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file, without the index\n",
    "df_final.to_csv(\"df_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc8b52f-752c-4727-9b2a-9be1fc50670f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = '508group'  # Replace with your bucket name\n",
    "file_name = \"df_final.csv\"  \n",
    "key = 'ADS508_project/cleandata/df_final.csv'  \n",
    "\n",
    "try:\n",
    "    s3.upload_file(file_name, bucket_name, key)\n",
    "    print(f\"File uploaded successfully to s3://{bucket_name}/{key}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"The file was not found\")\n",
    "except NoCredentialsError:\n",
    "    print(\"Credentials not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e21949-2cbd-4cf0-9ced-719399d5ed6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4ff34-d035-45da-996d-14faae391ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.automl.automl import AutoML\n",
    "\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# Replace 'your_dataset.csv' with the name of your CSV file\n",
    "input_data = 's3://508group/ADS508_project/cleandata/df_final.csv'\n",
    "\n",
    "autopilot_job = AutoML(\n",
    "    role=role,\n",
    "    target_attribute_name='sentimentoutcome',  # This is the column you're predicting\n",
    "    output_path='s3://508group/ADS508_project/output/autopilot/',\n",
    "    max_candidates=20,\n",
    "    sagemaker_session=session,\n",
    "    problem_type='MulticlassClassification',  # Assuming sentimentoutcome is multiclass\n",
    "    job_objective={'MetricName': 'Accuracy'}  # You can choose another metric if it fits better\n",
    ")\n",
    "\n",
    "autopilot_job.fit(inputs=input_data, wait=False, logs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee36678-3c53-4b6f-a991-5ab83495617c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94feeb6c-5710-40aa-add8-21333a57a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3 bucket and object key\n",
    "bucket_name = '508group'\n",
    "object_key = 'ADS508_project/output/autopilot/automl-2024-03-30-19-51-00-981/data-processor-models/automl-2024-03-30-19-51-00-981-dpp9-1-89acfb1b856c4754873c58ddd/output/model.tar.gz'\n",
    "\n",
    "# Local directory to extract the contents\n",
    "extract_dir = \"ADS508_project/output/autopilot/automl-2024-03-30-19-51-00-981/data-processor-models/automl-2024-03-30-19-51-00-981-dpp9-1-89acfb1b856c4754873c58ddd/output/model.tar.gz\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Download the file from S3\n",
    "local_file_path = os.path.join(extract_dir, 'model.tar.gz')\n",
    "s3.download_file(bucket_name, object_key, local_file_path)\n",
    "\n",
    "# Open the tar file and extract the contents\n",
    "with tarfile.open(local_file_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=extract_dir)\n",
    "\n",
    "# After extraction, you can further process or use the contents of the extracted files\n",
    "# For example, list the extracted files\n",
    "extracted_files = os.listdir(extract_dir)\n",
    "print(\"Extracted files:\", extracted_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa5f57-4ea1-4f18-bf70-6d1d232ad1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6873f18-501e-4edd-82d8-bca02cadce96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the file from S3\n",
    "local_file_path = os.path.join(extract_dir, 'model.tar.gz')\n",
    "s3.download_file(bucket_name, object_key, local_file_path)\n",
    "\n",
    "# Extract the contents of the tar.gz file\n",
    "with tarfile.open(local_file_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=extract_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc47c7d-67af-4ba9-ba7c-7c57c5c2025b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ac72b-3a64-4006-a16c-b53dc09f189b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfceb6d-0f2e-422d-bba1-60e70834b791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1476451-bf77-4e01-bb34-f89255e99bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install --upgrade numpy scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccec5d-a028-4d7c-b486-dc7edc52f6ad",
   "metadata": {},
   "source": [
    "# Data Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9632-a090-45b8-835f-6e9e9523239c",
   "metadata": {},
   "source": [
    "### Assign input and outcome variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a6ffd96-cee7-4aa1-88b9-f79ff82c4769",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df_final[['clean_text', 'state_id']]\n",
    "y = df_final['sentimentoutcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6711be-27cf-48c1-bb04-65dda756fc61",
   "metadata": {},
   "source": [
    "### Create a 90/5/5 data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "969a63d5-cf39-4d1c-b08b-ffcfbadbf373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into 90% train and 10% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "\n",
    "# Split remaining data (90% train) into 90% train and 10% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=1/9, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da2621-7fce-410f-b131-2798a1c7e0c7",
   "metadata": {},
   "source": [
    "### Further undersample the training set so that each level of the outcome variable is equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84c1c77f-9124-4fe2-8524-396167ba48f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install imblearn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Undersample the training set to balance the six categorical outcome levels\n",
    "undersampler = RandomUnderSampler(sampling_strategy='all', random_state=1)\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62afaf20-6937-46dd-8748-fb1fd9da27e1",
   "metadata": {},
   "source": [
    "## Verify split datasets size and datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "288f0ae6-b39a-446b-b2f7-44e4386bc6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 43860 entries, 53437 to 97982\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   clean_text  43860 non-null  object\n",
      " 1   state_id    43860 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train_resampled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab556b3e-f0d4-4da9-9e2d-d00637699f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update Datatypes\n",
    "X_train_resampled['clean_text'] = X_train_resampled['clean_text'].astype(str)\n",
    "X_train_resampled['state_id'] = X_train_resampled['state_id'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9970eab-cc66-4b88-ba99-b508cb3f72be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 43860 entries, 53437 to 97982\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype   \n",
      "---  ------      --------------  -----   \n",
      " 0   clean_text  43860 non-null  object  \n",
      " 1   state_id    43860 non-null  category\n",
      "dtypes: category(1), object(1)\n",
      "memory usage: 730.6+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train_resampled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ee121-0cb7-4372-b13a-b10d6ee72732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "816b5154-e57b-40d9-86cf-f4306ca60e00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 33025 entries, 124161 to 206753\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   clean_text  33025 non-null  object\n",
      " 1   state_id    33025 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 774.0+ KB\n"
     ]
    }
   ],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26b725c5-5333-40f7-af2a-a9f21ecb1d3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update Datatypes\n",
    "X_test['clean_text'] = X_test['clean_text'].astype(str)\n",
    "X_test['state_id'] = X_test['state_id'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e19e81d-455a-404b-9cf2-b888db8a847a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 33025 entries, 237346 to 197012\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   clean_text  33025 non-null  object\n",
      " 1   state_id    33025 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 774.0+ KB\n"
     ]
    }
   ],
   "source": [
    "X_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfdc91ec-5853-4fde-bb78-47583fc58c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update Datatypes\n",
    "X_val['clean_text'] = X_val['clean_text'].astype(str)\n",
    "X_val['state_id'] = X_val['state_id'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0d6be10-6202-4a24-a9f3-99a36fa48b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43860, 2), (33025, 2), (33025, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resampled.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3bd9f63-cf44-48c7-b4ea-38de43c52cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>state_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53437</th>\n",
       "      <td>If his base can still hate Biden after all th...</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113899</th>\n",
       "      <td>Biden didn t run against socialists There wer...</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97700</th>\n",
       "      <td>RepDougCollins Oh screw off PresidentElect Jo...</td>\n",
       "      <td>IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106233</th>\n",
       "      <td>Lynn Biden voted for the bankruptcy bill that...</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160694</th>\n",
       "      <td>Joe Biden I believe being accountable means h...</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean_text state_id\n",
       "53437    If his base can still hate Biden after all th...       NY\n",
       "113899   Biden didn t run against socialists There wer...       PA\n",
       "97700    RepDougCollins Oh screw off PresidentElect Jo...       IL\n",
       "106233   Lynn Biden voted for the bankruptcy bill that...       PA\n",
       "160694   Joe Biden I believe being accountable means h...       OH"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec546bbf-599d-4d64-bdb4-ac1823fdc320",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>state_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124161</th>\n",
       "      <td>As Trump rids himself and the country of resp...</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>Joe Biden needs to sign up for anti racism tra...</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291818</th>\n",
       "      <td>I saw a Trump banner on a lawn the other day</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98845</th>\n",
       "      <td>Trump was caught by the virus and his incompet...</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207321</th>\n",
       "      <td>Trump s lawyers saw this coming Now we know wh...</td>\n",
       "      <td>VA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean_text state_id\n",
       "124161   As Trump rids himself and the country of resp...       MA\n",
       "3351    Joe Biden needs to sign up for anti racism tra...       NY\n",
       "291818       I saw a Trump banner on a lawn the other day       MA\n",
       "98845   Trump was caught by the virus and his incompet...       GA\n",
       "207321  Trump s lawyers saw this coming Now we know wh...       VA"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9ba7754-ad48-4e25-a942-d189c6e29e71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>state_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>237346</th>\n",
       "      <td>The attempt to cover up the latest covid 19 v...</td>\n",
       "      <td>RI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57321</th>\n",
       "      <td>In defense of at least some Trump supporters</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276637</th>\n",
       "      <td>The only remote chance of this happening is i...</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10263</th>\n",
       "      <td>Trump is seemingly having a meltdown which wi...</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313099</th>\n",
       "      <td>I filled out my ballot for Bloomberg but then...</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean_text state_id\n",
       "237346   The attempt to cover up the latest covid 19 v...       RI\n",
       "57321        In defense of at least some Trump supporters       CO\n",
       "276637   The only remote chance of this happening is i...       TX\n",
       "10263    Trump is seemingly having a meltdown which wi...       CO\n",
       "313099   I filled out my ballot for Bloomberg but then...       CO"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d948f1-46dc-49bb-aebc-0815fb9a0de4",
   "metadata": {},
   "source": [
    "## Verify balance of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c30dc150-b36e-4f05-aa37-f608aadd1e38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trump_nega    7310\n",
       "Trump_posi    7310\n",
       "Biden_nega    7310\n",
       "Biden_neut    7310\n",
       "Biden_posi    7310\n",
       "Trump_neut    7310\n",
       "Name: sentimentoutcome, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74953e5a-72fe-4574-9162-43840176bd6d",
   "metadata": {},
   "source": [
    "## Average length of comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3bda01-4fb0-4dd5-af07-f38ae678d7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to count string word length\n",
    "def count_word_length(text):\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "# Add column to DataFrame\n",
    "df_final['word_length'] = df_final['clean_text'].apply(lambda x: count_word_length(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b161f93-9d09-4588-9f9f-7e9e43b4533d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "median_word_length = np.median(df_final['word_length'])\n",
    "\n",
    "# Plot histogram of word lengths\n",
    "plt.hist(df_final['word_length'], bins=range(min(df_final['word_length']), max(df_final['word_length']) + 1), edgecolor='black')\n",
    "plt.axvline(median_word_length, color='red', linestyle='dashed', linewidth=1)  # Add vertical line at median\n",
    "plt.text(median_word_length, plt.ylim()[1] * 0.9, f'Median: {median_word_length:.2f}', color='red')  # Add label for median\n",
    "plt.xlabel('Word Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Word Lengths')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e6a97-9067-4217-b9c1-fddcfd7cb39d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "923478e9-afa0-4267-bbd9-23eb49366603",
   "metadata": {},
   "source": [
    "# Text Entry Preprocessing and Data Transformation Pipeline: Will Need to Add Categorical One-Hot Encoding if we use Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceecb0fe-e564-4f77-9d5f-69a3ba984adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#from sklearn.base import BaseEstimator, TransformerMixin\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from gensim.models import Word2Vec\n",
    "\n",
    "# Define custom transformer for Word2Vec embeddings\n",
    "#class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "#    def __init__(self, window=5, min_count=1, workers=4):\n",
    "#        self.word2vec_model = Word2Vec(window=window, min_count=min_count, workers=workers)\n",
    "#    \n",
    "#    def fit(self, X, y=None):\n",
    "#        # Train Word2Vec model\n",
    "#        X_word_lists = [text.split()[:15] for text in X]  # Limit to first 15 words\n",
    "#        self.word2vec_model.build_vocab(X_word_lists)\n",
    "#        self.word2vec_model.train(X_word_lists, total_examples=len(X_word_lists), epochs=10)\n",
    "#        return self\n",
    "#    \n",
    "#    def transform(self, X):\n",
    "#        # Limit text to first 15 words and split into word lists\n",
    "#        X_word_lists = [text.split()[:15] for text in X]\n",
    "#        # Initialize a list to store word embeddings\n",
    "#        embeddings = []\n",
    "#        # Iterate over each word list\n",
    "#        for word_list in X_word_lists:\n",
    "#            # Initialize a list to store word embeddings for this word list\n",
    "#            word_embeddings = []\n",
    "#            # Iterate over each word in the word list\n",
    "#            for word in word_list:\n",
    "                # Check if the word exists in the Word2Vec model's vocabulary\n",
    "#                if word in self.word2vec_model.wv:\n",
    "#                    # Retrieve the word embedding and append it to the list\n",
    "#                    word_embeddings.append(self.word2vec_model.wv[word])\n",
    "#                else:\n",
    "#                    # If the word doesn't exist in the vocabulary, append zeros\n",
    "#                    word_embeddings.append([0] * self.word2vec_model.vector_size)\n",
    "#            # Append the word embeddings for this word list to the embeddings list\n",
    "#            embeddings.append(word_embeddings)\n",
    "#        return embeddings\n",
    "\n",
    "# Define preprocessing pipeline\n",
    "#text_pipeline = Pipeline([\n",
    "#    ('word2vec', Word2VecTransformer())\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde8566-8f5a-45f5-867f-17afcd7ba631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit and transform on the training set\n",
    "#X_train_transformed_text = pd.DataFrame(text_pipeline.fit_transform(X_train_resampled['clean_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4a9a8b-4344-40f8-b409-65ce5fd5b980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X_train_transformed_state = pd.DataFrame(cat_pipeline.fit_transform(X_train_resampled))\n",
    "#X_train_transformed_state.columns = ['state_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701c679e-11ba-4778-ab06-8a1e36dc7af0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X_train_transformed_text.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb8f70b-d846-43a6-9ffd-958b416762f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X_train_transformed_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4481089-0717-40f9-903d-ea86511c3a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X_train = pd.concat([X_train_transformed_text, X_train_transformed_state], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf3146-361b-40de-8172-f6a7ddacdf74",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b7aaf-71f5-47de-8fc5-201c388fd6cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a63cc4f-4fe6-4ca4-957a-231835b072a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f849e6-fa6b-4951-a7c9-191be9bb2bd3",
   "metadata": {},
   "source": [
    "### Trying a different Preprocess, since I was having trouble with Word2Vec (This step ignores anything after word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab265e3-982d-4b30-bf79-716ca11edbab",
   "metadata": {},
   "source": [
    "### First function using \"last_hidden_state\" which did not work well with my PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551b056-465c-420c-92b3-cb63e35ec549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class DistilBERTTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # Initialize the DistilBERT tokenizer and model\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, batch_size=32):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        embeddings = []\n",
    "        for i in tqdm(range(0, len(X), batch_size)):\n",
    "            batch = X[i:i+batch_size]\n",
    "            inputs = self.tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            embeddings.append(outputs.last_hidden_state.mean(dim=1).detach().cpu().numpy())  # Use mean pooling\n",
    "        return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3c0d55-c87b-413e-8514-194f1e45995e",
   "metadata": {},
   "source": [
    "### Second function directly appends outputs to embeddings without using last_hidden_state. Also shorted max_length of input characters to 64 to reduce computational overload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8224bb6f-d1be-4e64-958c-16258016af03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class DistilBERTTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # Initialize the DistilBERT tokenizer and model\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, batch_size=32):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        embeddings = []\n",
    "        for i in tqdm(range(0, len(X), batch_size)):\n",
    "            batch = X[i:i+batch_size]\n",
    "            inputs = self.tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=64).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            # Adjust based on the actual structure of the outputs\n",
    "            embeddings.append(outputs[0].mean(dim=1).detach().cpu().numpy())  # Use mean pooling\n",
    "        return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9b3cef-c2f2-4bac-9d0e-8ed7f5345a77",
   "metadata": {},
   "source": [
    "### BERT transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0f42411-f1fe-4448-a08f-0e939341d630",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1371/1371 [39:07<00:00,  1.71s/it]\n"
     ]
    }
   ],
   "source": [
    "distilbert_transformer = DistilBERTTransformer()\n",
    "# Transform 'clean_text' into DistilBERT embeddings for the training set\n",
    "texts_train = X_train_resampled['clean_text'].tolist()  # Extract texts as a list\n",
    "distilbert_embeddings_train = distilbert_transformer.transform(texts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffbbb5e-9f65-4c7c-879a-7216db1219de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 126/1033 [03:33<26:10,  1.73s/it]"
     ]
    }
   ],
   "source": [
    "# Transform 'clean_text' into DistilBERT embeddings for the validation set\n",
    "texts_val = X_val['clean_text'].tolist()  # Extract texts as a list\n",
    "distilbert_embeddings_val = distilbert_transformer.transform(texts_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4499044-70a3-4a9d-a04c-8f7dc565e845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform 'clean_text' into DistilBERT embeddings for the test set\n",
    "texts_test = X_test['clean_text'].tolist()  # Extract texts as a list\n",
    "distilbert_embeddings_test = distilbert_transformer.transform(texts_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0055a819-ac45-485f-b6d8-8dd3aba6ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_embeddings_train = pd.DataFrame(distilbert_embeddings_train)\n",
    "distilbert_embeddings_val = pd.DataFrame(distilbert_embeddings_val)\n",
    "distilbert_embeddings_test = pd.DataFrame(distilbert_embeddings_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5ffeb-32ed-4cf7-8c30-cf4d62a94aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_embeddings_train['dataset'] = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52466131-5f4d-4fb0-9c45-aa25ea91413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_embeddings_val['dataset'] = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55cb760-aa61-4b14-a0fe-de79a33229d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_embeddings_test['dataset'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a296a79-a0da-4d7f-9959-7fa26c3efd37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.095015</td>\n",
       "      <td>0.009549</td>\n",
       "      <td>0.009942</td>\n",
       "      <td>0.037959</td>\n",
       "      <td>0.117014</td>\n",
       "      <td>-0.110813</td>\n",
       "      <td>0.070921</td>\n",
       "      <td>0.102977</td>\n",
       "      <td>-0.001028</td>\n",
       "      <td>-0.204761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149287</td>\n",
       "      <td>0.135476</td>\n",
       "      <td>0.210159</td>\n",
       "      <td>-0.067416</td>\n",
       "      <td>0.237532</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>0.022339</td>\n",
       "      <td>0.059760</td>\n",
       "      <td>0.073708</td>\n",
       "      <td>-0.043815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.030366</td>\n",
       "      <td>0.011380</td>\n",
       "      <td>-0.126178</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>-0.143618</td>\n",
       "      <td>-0.003824</td>\n",
       "      <td>-0.047498</td>\n",
       "      <td>0.257969</td>\n",
       "      <td>-0.085199</td>\n",
       "      <td>-0.151125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017310</td>\n",
       "      <td>0.081773</td>\n",
       "      <td>-0.080077</td>\n",
       "      <td>-0.029285</td>\n",
       "      <td>0.260058</td>\n",
       "      <td>-0.097784</td>\n",
       "      <td>0.064009</td>\n",
       "      <td>-0.055928</td>\n",
       "      <td>0.078552</td>\n",
       "      <td>0.035350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.095015  0.009549  0.009942  0.037959  0.117014 -0.110813  0.070921   \n",
       "1 -0.030366  0.011380 -0.126178  0.100002 -0.143618 -0.003824 -0.047498   \n",
       "\n",
       "        7         8         9    ...       758       759       760       761  \\\n",
       "0  0.102977 -0.001028 -0.204761  ...  0.149287  0.135476  0.210159 -0.067416   \n",
       "1  0.257969 -0.085199 -0.151125  ...  0.017310  0.081773 -0.080077 -0.029285   \n",
       "\n",
       "        762       763       764       765       766       767  \n",
       "0  0.237532  0.094727  0.022339  0.059760  0.073708 -0.043815  \n",
       "1  0.260058 -0.097784  0.064009 -0.055928  0.078552  0.035350  \n",
       "\n",
       "[2 rows x 768 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(distilbert_embeddings_train)\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f22603-f1fe-43d9-8563-4060acfba226",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_embeddings_val.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2864a4-6921-4b69-a768-9c4a397b8155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distilbert_embeddings_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd731b1-8c48-4454-99d6-994fa60c9343",
   "metadata": {},
   "source": [
    "### Store Transformed BERT Datasets in the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14d46d-14f2-4d42-969c-648c488d938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Train, Val, and Test BERT data into One Dataframe\n",
    "all_embeddings = np.vstack((distilbert_embeddings_train, distilbert_embeddings_val, distilbert_embeddings_test))\n",
    "\n",
    "# Add unique comment ID\n",
    "all_embeddings['comment_id'] = range(1, len(all_embeddings) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34daa9e3-3472-4177-aa78-015143c42431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "feature_name = 'comments_distilbert_max_64'\n",
    "feature_group = FeatureGroup(name=feature_name, sagemaker_session=sagemaker_session)\n",
    "record_identifier_feature_name = 'comment_id'\n",
    "event_time_feature_name = 'date'\n",
    "prefix = \"ADS508_project/cleandata/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c37d1-5dd2-4b81-ae17-3d93b79e4ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group.create(s3_uri = \"s3://{}/{}\".format(bucket, prefix),\n",
    "                    record_identifier_name = record_identifier_feature_name,\n",
    "                    event_time_feature_name = event_time_feature_name,\n",
    "                    role_arn = role,\n",
    "                    enable_online_store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61cf51b-f09a-4cb6-b8f1-cfadc273e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group.ingest(data_frame=all_embeddings, max_workers=3, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73ab26a-66ac-423e-8ea3-edb87fd8ad62",
   "metadata": {},
   "source": [
    "## Create Datasets with just BERT Text Data: Retrieve Data From Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ad4c3-fab0-4d47-b688-fd947c8a1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_query = feature_group.athena_query()\n",
    "feature_table = feature_query.table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ed4386-8cb5-4828-896c-a84fdae83d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query = \"\"\"\n",
    "SELECT input_ids FROM {} WHERE dataset='train'\n",
    "\"\"\".format(feature_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bdeb79-55d2-416f-a8ff-b28caeccbb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bert = pd.DataFrame(feature_query.run(query_string=train_query))\n",
    "X_train_bert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77b6dd-e9c2-43da-8615-80b5ddb1252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bert = pd.read_sql(train_query, conn)\n",
    "X_train_bert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101f1194-9f75-49cc-a91c-4458b8bc9592",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_query = \"\"\"\n",
    "SELECT input_ids FROM {} WHERE dataset='val'\n",
    "\"\"\".format(feature_query)\n",
    "\n",
    "X_train_val = pd.DataFrame(feature_query.run(query_string=val_query))\n",
    "X_train_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff788cd7-338d-480a-b68a-cc82cfbc513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"\"\"\n",
    "SELECT input_ids FROM {} WHERE dataset='test'\n",
    "\"\"\".format(feature_query)\n",
    "\n",
    "X_train_test = pd.DataFrame(feature_query.run(query_string=test_query))\n",
    "X_train_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c4f70-c426-46c8-92f4-3cb69c8a6b21",
   "metadata": {},
   "source": [
    "## Create Datasets with both BERT Text Data and One-Hot Encoded Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a4019-77f7-4bd3-b09b-34d4ae28dc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False) \n",
    "\n",
    "# Assuming X_train, X_val, X_test are your datasets\n",
    "state_ids_train = X_train['state_id'].values.reshape(-1, 1)  # Reshape for the encoder\n",
    "state_id_encoded_train = onehot_encoder.fit_transform(state_ids_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4784e033-9ce5-4856-8e89-9f7c99bca033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_ids_val = X_val['state_id'].values.reshape(-1, 1)\n",
    "state_id_encoded_val = onehot_encoder.transform(state_ids_val)\n",
    "\n",
    "state_ids_test = X_test['state_id'].values.reshape(-1, 1)\n",
    "state_id_encoded_test = onehot_encoder.transform(state_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec49c47-7eb0-4fbe-bf9c-f8922b9c8ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Combine with sate ID that is one hot encoded\n",
    "X_train_prepared = np.hstack((X_train_bert, state_id_encoded_train))\n",
    "X_val_prepared = np.hstack((X_val_bert, state_id_encoded_val))\n",
    "X_test_prepared = np.hstack((X_test_bert, state_id_encoded_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10bbe96-dd69-4011-ae55-34ff8c57771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce88c5-1dfd-4056-916f-4210192f260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ed7fd7-7898-48a5-b71d-946fc5180eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prepared.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82085ec2-8da9-498c-a3ac-b0d405d6bb36",
   "metadata": {},
   "source": [
    "### Drop C-1 Dummies for Baseline Linear Models (i.e., logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f333c2f-f55c-4fb7-b28c-ff7b9f2ad270",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_both = X_train_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1846b-587d-4584-b3f3-a2831102d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_both = X_val_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777af846-3611-4e48-888c-88d4bd09bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_both = X_test_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b3a58d-f94c-414e-81ab-d8036d2be11c",
   "metadata": {},
   "source": [
    "## Baseline 1: Logistic Regression with only BERT Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3198671-18ce-4d5c-84a4-cb5c70b7c0db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg1 = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "\n",
    "# Fit the model on the balanced text training data\n",
    "log_reg1.fit(X_train_bert, y_train_resampled)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = log_reg1.predict(X_val_bert)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Baseline 1: Validation Set Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(\"\\nBasline 1: Validation Set Classification Report:\\n\", classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8b4bcb-e461-4a63-8e28-f73c7510675b",
   "metadata": {},
   "source": [
    "## Baseline 2: Logistic Regression with Text Data and Location (c-1 dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b458aab-a61a-4879-83a0-b2537b44d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg2 = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "\n",
    "# Fit the model on the balanced text training data\n",
    "log_reg2.fit(X_train_both, y_train_resampled)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = log_reg2.predict(X_val_both)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Baseline 2: Validation Set Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(\"\\nBaselin 2: Validation Set Classification Report:\\n\", classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ea87c-8c4a-4b27-b572-a9080854cc7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MLP Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ba8d1-8872-4883-b77f-b8858b8fe275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Use a pipeline to standardize features\n",
    "mlp_pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, activation='relu', solver='adam', verbose=True, tol=0.001, alpha=0.0001)\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "mlp_pipeline.fit(X_train_prepared, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_val_pred = mlp_pipeline.predict(X_val_prepared)\n",
    "print(\"Validation Set Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(\"Validation Set Classification Report:\\n\", classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292bd6ce-e78b-4ef1-a244-897f84c8594e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbd3687d-311a-4be5-aba4-b879dbaaf2e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df51ce2e-9fd9-4695-8891-d6e3576e1fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Initialize the MLP Classifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=300, activation='relu', solver='adam', verbose=True, tol=0.01)\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "mlp.fit(X_train_prepared, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred_mlp = mlp.predict(X_val_prepared)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Validation Set Accuracy (MLP):\", accuracy_score(y_val, y_val_pred_mlp))\n",
    "print(\"\\nValidation Set Classification Report (MLP):\\n\", classification_report(y_val, y_val_pred_mlp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9952871e-0de0-488e-afdb-269926bd1470",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81738c07-0452-408d-b5d7-c6dcf4733ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define device upfront\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming X_train_prepared, X_val_prepared, X_test_prepared and their respective y's are already defined\n",
    "\n",
    "# Encode labels numerically\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"features\": torch.tensor(self.features[idx], dtype=torch.float),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = CustomDataset(X_train_prepared, y_train_encoded)\n",
    "val_dataset = CustomDataset(X_val_prepared, y_val_encoded)\n",
    "test_dataset = CustomDataset(X_test_prepared, y_test_encoded)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Model Definition\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Classifier(input_size=X_train_prepared.shape[1], num_classes=len(set(y_train_encoded)))\n",
    "model.to(device)\n",
    "\n",
    "# Training\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch['features'].to(device), batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        features, labels = batch['features'].to(device), batch['labels'].to(device)\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139a3eb-a75f-47d6-9738-60e88975954b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d7a81-5a07-4ae8-b0d9-4daa9c6de530",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import pandas as pd\n",
    "# from io import StringIO\n",
    "\n",
    "# # Create an S3 client\n",
    "# s3 = boto3.client('s3')\n",
    "\n",
    "# # Specify the name of the bucket\n",
    "# bucket_name = '508group'\n",
    "\n",
    "# # Ensure your DataFrame variables are defined here (X_train, y_train_resampled, X_test, y_test, X_val, y_val)\n",
    "\n",
    "# # Define the DataFrames\n",
    "# data_frames = {\n",
    "#     'X_train': X_train,\n",
    "#     'y_train_resampled': y_train_resampled,\n",
    "#     'X_test': X_test,\n",
    "#     'y_test': y_test,\n",
    "#     'X_val': X_val,\n",
    "#     'y_val': y_val,\n",
    "# }\n",
    "\n",
    "# # Upload each DataFrame as CSV to the \"508group\" bucket\n",
    "# for key, df in data_frames.items():\n",
    "#     try:\n",
    "#         # Convert DataFrame to CSV string\n",
    "#         csv_buffer = StringIO()\n",
    "#         df.to_csv(csv_buffer, index=False)\n",
    "        \n",
    "#         # Upload CSV string to S3\n",
    "#         s3.put_object(Body=csv_buffer.getvalue(), Bucket=bucket_name, Key=f'ADS508_project/cleandata/{key}.csv')\n",
    "#         print(f\"Successfully uploaded {key}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error uploading {key} to S3: {e}\")\n",
    "\n",
    "# # Define the S3 paths for the data now that they are uploaded\n",
    "# s3_paths = {\n",
    "#     'X_train': 's3://508group/ADS508_project/cleandata/X_train.csv',\n",
    "#     'y_train_resampled': 's3://508group/ADS508_project/cleandata/y_train_resampled.csv',\n",
    "#     'X_test': 's3://508group/ADS508_project/cleandata/X_test.csv',\n",
    "#     'y_test':  's3://508group/ADS508_project/cleandata/y_test.csv',\n",
    "#     'X_val': 's3://508group/ADS508_project/cleandata/X_val.csv',\n",
    "#     'y_val': 's3://508group/ADS508_project/cleandata/y_val.csv',\n",
    "# }\n",
    "# output_path = 's3://508group/ADS508_project/output/'\n",
    "\n",
    "# You can now use `s3_paths` dictionary to access your data in S3 for any further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05531a56-8cb8-4a24-85f4-c11a373cad2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce5cf89-ad04-4625-92e3-c621b175aee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff2b88-cc09-4078-b3af-5c5c341fa58b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import pandas as pd\n",
    "# from io import StringIO\n",
    "\n",
    "# # Create an S3 client\n",
    "# s3 = boto3.client('s3')\n",
    "\n",
    "# # Specify the name of the bucket\n",
    "# bucket_name = '508group'\n",
    "\n",
    "# # Define the DataFrames\n",
    "# data_frames = {\n",
    "#     'X_train': X_train,\n",
    "#     'y_train_resampled': y_train_resampled,\n",
    "#     'X_test': X_test,\n",
    "#     'y_test': y_test,\n",
    "#     'X_val': X_val,\n",
    "#     'y_val': y_val,\n",
    "# }\n",
    "\n",
    "# # Upload each DataFrame as CSV to the \"508group\" bucket\n",
    "# for key, df in data_frames.items():\n",
    "#     try:\n",
    "#         # Convert DataFrame to CSV string\n",
    "#         csv_buffer = StringIO()\n",
    "#         df.to_csv(csv_buffer, index=False)\n",
    "        \n",
    "#         # Upload CSV string to S3\n",
    "#         s3.put_object(Body=csv_buffer.getvalue(), Bucket=bucket_name, Key=f'ADS508_project/cleandata/{key}.csv')\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error uploading {key} to S3: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e7cd7d-d7d9-4e0b-9503-4c282d2d0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the S3 paths for training data, model output, etc.\n",
    "# X_train = 's3://508group/ADS508_project/cleandata/X_train.csv'\n",
    "# y_train_resampled = 's3://508group/ADS508_project/cleandata/y_train_resampled.csv'\n",
    "# X_test = 's3://508group/ADS508_project/cleandata/X_test.csv'\n",
    "# y_test =  's3://508group/ADS508_project/cleandata/y_test.csv'\n",
    "# X_val = 's3://508group/ADS508_project/cleandata/X_val.csv'\n",
    "# y_val = 's3://508group/ADS508_project/cleandata/y_val.csv'\n",
    "# output_path = 's3://508group/ADS508_project/output/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb6ffd-707d-4bff-9dae-042b4eafd0ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ed65f-9093-40d8-98da-30f26dcb896d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sagemaker\n",
    "# from sagemaker import get_execution_role\n",
    "# from sagemaker.inputs import TrainingInput\n",
    "# from sagemaker.estimator import Estimator\n",
    "\n",
    "# # Step 1: Define S3 paths for training data, model output, etc.\n",
    "# s3_input_train = 's3://508group/ADS508_project/cleandata/X_train.csv'\n",
    "# s3_input_test = 's3://508group/ADS508_project/cleandata/X_test.csv'\n",
    "# output_path = 's3://508group/ADS508_project/output/'\n",
    "\n",
    "# # Step 2: Create a SageMaker session and specify the IAM role\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "# role = get_execution_role()\n",
    "\n",
    "# # Step 3: Define the SageMaker XGBoost estimator with the specific version and hyperparameters\n",
    "# xgb_estimator = Estimator(image_uri=sagemaker.image_uris.retrieve(\"xgboost\", sagemaker_session.boto_region_name, \"1.7-1\"),\n",
    "#                           role=role,\n",
    "#                           instance_count=1,\n",
    "#                           instance_type='ml.m5.large',\n",
    "#                           output_path=output_path,\n",
    "#                           sagemaker_session=sagemaker_session,\n",
    "#                           hyperparameters={'num_round': '10'})  # Specify the number of boosting rounds\n",
    "\n",
    "# # Step 4: Train the XGBoost model\n",
    "# xgb_estimator.fit({'train': s3_input_train, 'validation': s3_input_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c38f2f-07f1-4ffe-9ec4-63eabbb042a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc54a5e9-1400-4bbd-84d5-67daacb2e4d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import os\n",
    "\n",
    "# # Initialize a boto3 client\n",
    "# s3 = boto3.client('s3')\n",
    "\n",
    "# # Function to download a file from an S3 bucket\n",
    "# def download_file(bucket_name, object_key, local_filename):\n",
    "#     try:\n",
    "#         s3.download_file(Bucket=bucket_name, Key=object_key, Filename=local_filename)\n",
    "#         print(f\"Downloaded {object_key} to {local_filename}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error downloading {object_key}: {e}\")\n",
    "\n",
    "# # Define your bucket name\n",
    "# bucket_name = '508group'\n",
    "\n",
    "# # List of artifacts to download\n",
    "# artifacts = {\n",
    "#     \"feature_engineering_code\": \"ADS508_project/output/autopilot/automl-2024-03-30-19-48-01-733/sagemaker-automl-candidates/automl-2024-03-30-19-48-01-733-pr-1-5cdcc23566e2483f97658942125/generated_module/candidate_data_processors/dpp9.py\",\n",
    "#     # Add other artifacts here following the same structure\n",
    "# }\n",
    "\n",
    "# # Download each artifact\n",
    "# for name, key in artifacts.items():\n",
    "#     download_file(bucket_name, key, f\"{name}.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b2d6c4-dfbb-4eb7-b2ec-ba49f136f650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import os\n",
    "\n",
    "# # Define key for the feature engineering code\n",
    "# fe_code_key = 'ADS508_project/output/autopilot/automl-2024-03-30-19-48-01-733/sagemaker-automl-candidates/automl-2024-03-30-19-48-01-733-pr-1-5cdcc23566e2483f97658942125/generated_module/candidate_data_processors/dpp3.py'\n",
    "# download_from_s3(bucket_name, fe_code_key, 'dpp3.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ffa2d-b300-4615-8249-5e1e732a9fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import os\n",
    "\n",
    "# s3 = boto3.client('s3')\n",
    "\n",
    "# def download_from_s3(bucket_name, s3_key, local_path):\n",
    "#     try:\n",
    "#         s3.download_file(bucket_name, s3_key, local_path)\n",
    "#         print(f\"Downloaded {s3_key} to {local_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error downloading {s3_key}: {e}\")\n",
    "\n",
    "# bucket_name = '508group' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae267212-beb5-49be-bced-3f8957b08d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import pandas as pd\n",
    "# from io import StringIO\n",
    "\n",
    "# # Create an S3 client\n",
    "# s3 = boto3.client('s3')\n",
    "\n",
    "# # Specify the name of the bucket\n",
    "# bucket_name = '508group'\n",
    "\n",
    "# # Define the DataFrames\n",
    "# data_frames = {\n",
    "#     'X_train': X_train,\n",
    "#     'y_train_resampled': y_train_resampled,\n",
    "#     'X_test': X_test,\n",
    "#     'y_test': y_test,\n",
    "#     'X_val': X_val,\n",
    "#     'y_val': y_val,\n",
    "# }\n",
    "\n",
    "# # Upload each DataFrame as CSV to the \"508group\" bucket\n",
    "# for key, df in data_frames.items():\n",
    "#     try:\n",
    "#         # Convert DataFrame to CSV string\n",
    "#         csv_buffer = StringIO()\n",
    "#         df.to_csv(csv_buffer, index=False)\n",
    "        \n",
    "#         # Upload CSV string to S3\n",
    "#         s3.put_object(Body=csv_buffer.getvalue(), Bucket=bucket_name, Key=f'ADS508_project/cleandata/{key}.csv')\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error uploading {key} to S3: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb0f9dd-2f59-4fdb-8756-9bc87e085014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sagemaker\n",
    "# from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# # Define your SageMaker session\n",
    "# sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c52b9-5fa1-4660-aaa3-09761b2c9a17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff76fbc-22b0-42d2-ba46-ef41ceea26c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Define the S3 paths for training data, model output, etc.\n",
    "# X_train = 's3://508group/ADS508_project/cleandata/X_train.csv'\n",
    "# y_train_resampled = 's3://508group/ADS508_project/cleandata/y_train_resampled.csv'\n",
    "# X_test = 's3://508group/ADS508_project/cleandata/X_test.csv'\n",
    "# y_test =  's3://508group/ADS508_project/cleandata/y_test.csv'\n",
    "# X_val = 's3://508group/ADS508_project/cleandata/X_val.csv'\n",
    "# y_val = 's3://508group/ADS508_project/cleandata/y_val.csv'\n",
    "# output_path = 's3://508group/ADS508_project/output/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e0114-1302-4d00-8a51-0803bfd777b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Load y_train_resampled.csv\n",
    "# y_train_resampled_df = pd.read_csv('s3://508group/ADS508_project/cleandata/y_train_resampled.csv')\n",
    "\n",
    "# # Prefix labels with '__label__'\n",
    "# y_train_resampled_df['sentimentoutcome'] = '__label__' + y_train_resampled_df['sentimentoutcome'].astype(str)\n",
    "\n",
    "# # Save the modified DataFrame back to y_train_resampled.csv\n",
    "# y_train_resampled_df.to_csv('s3://508group/ADS508_project/cleandata/y_train_resampled.csv', index=False)\n",
    "\n",
    "\n",
    "# # Load y_val.csv\n",
    "# y_val_df = pd.read_csv('s3://508group/ADS508_project/cleandata/y_val.csv')\n",
    "\n",
    "# # Prefix labels with '__label__'\n",
    "# y_val_df['sentimentoutcome'] = '__label__' + y_val_df['sentimentoutcome'].astype(str)\n",
    "\n",
    "# # Save the modified DataFrame back to y_val.csv\n",
    "# y_val_df.to_csv('s3://508group/ADS508_project/cleandata/y_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83378cc8-c7d7-4597-bd2e-7fb89c0a7764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f011adc-ecd3-45d8-998a-98a29ffcf3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import boto3\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from joblib import dump\n",
    "# import os\n",
    "\n",
    "# # Load data from S3\n",
    "# s3 = boto3.client('s3')\n",
    "# bucket_name = '508group'\n",
    "\n",
    "# def load_data_from_s3(key):\n",
    "#     response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "#     data = pd.read_csv(response['Body'])\n",
    "#     return data\n",
    "\n",
    "# # Load training data\n",
    "# X_train = load_data_from_s3('ADS508_project/cleandata/X_train.csv')\n",
    "# y_train = load_data_from_s3('ADS508_project/cleandata/y_train_resampled.csv')\n",
    "\n",
    "# # Train logistic regression model\n",
    "# model = LogisticRegression()\n",
    "# model.fit(X_train_final, y_train_final)\n",
    "\n",
    "# # Evaluate model\n",
    "# y_pred = model.predict(X_val)\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# classification_rep = classification_report(y_val, y_pred)\n",
    "\n",
    "# # Save the trained model\n",
    "# output_dir = 'model'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# model_path = os.path.join(output_dir, 'model.joblib')\n",
    "# dump(model, model_path)\n",
    "\n",
    "# # Save evaluation metrics\n",
    "# metrics_path = os.path.join(output_dir, 'metrics.txt')\n",
    "# with open(metrics_path, 'w') as f:\n",
    "#     f.write(f'Accuracy: {accuracy}\\n')\n",
    "#     f.write('Classification Report:\\n')\n",
    "#     f.write(classification_rep)\n",
    "\n",
    "# # Upload the trained model and metrics to S3\n",
    "# s3.upload_file(model_path, bucket_name, 'ADS508_project/output/model.joblib')\n",
    "# s3.upload_file(metrics_path, bucket_name, 'ADS508_project/output/metrics.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b601c5e3",
   "metadata": {},
   "source": [
    "## Run Data Bias Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d4c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder for specific timestamp\n",
    "import time\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "bias_data_s3_uri = sess.upload_data(bucket=bucket, key_prefix=\"ADS508_project/bias-detection-{}\".format(timestamp), path=path)\n",
    "bias_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b272d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $bias_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a14b90-8b53-44ff-af78-361467ec987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_bias_data_s3_uri = sess.upload_data(\n",
    "    bucket=bucket, key_prefix=\"ADS508_project/bias-detection-{}\".format(timestamp), path=path_balance\n",
    ")\n",
    "balanced_bias_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea3079-9390-4130-964a-7b5e788f9523",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $balanced_bias_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6837356-c41b-4559-834c-68b3c576e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_bias_data_jsonlines_s3_uri = sess.upload_data(\n",
    "    bucket=bucket, key_prefix=\"ADS508_project/bias-detection-{}\".format(timestamp), path=path_jsonlines\n",
    ")\n",
    "balanced_bias_data_jsonlines_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28374d-24a2-49bf-8a5b-6899db532035",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $balanced_bias_data_jsonlines_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e829b584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --force-reinstall -q smclarify\n",
    "\n",
    "from smclarify.bias import report\n",
    "from typing import Dict\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c9cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $bias_data_s3_uri ./data-clarify/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee23cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $balanced_bias_data_s3_uri ./data-clarify/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94295cea",
   "metadata": {},
   "source": [
    "### Calculate Bias Metrics for all Data\n",
    "#### might not needed from ===>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533ede4d",
   "metadata": {},
   "source": [
    "facet_column = report.FacetColumn(name=\"candidatepoll\")\n",
    "\n",
    "label_column = report.LabelColumn(\n",
    "    name=\"sentiment_category\", \n",
    "    series=df_combined[\"sentiment_category\"],\n",
    "    positive_label_values=[5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36107b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.bias_report(\n",
    "    df=df_combined, \n",
    "    facet_column=facet_column, \n",
    "    label_column=label_column, \n",
    "    stage_type=report.StageType.PRE_TRAINING, \n",
    "    metrics=[\"CI\", \"DPL\", \"KL\", \"JS\", \"LP\", \"TVD\", \"KS\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff466df",
   "metadata": {},
   "source": [
    "### Calculate Bias Metrics for Balanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f57ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smclarify.bias import report\n",
    "\n",
    "facet_column = report.FacetColumn(name=\"candidatepoll\")\n",
    "\n",
    "label_column = report.LabelColumn(\n",
    "    name=\"sentiment_category\", \n",
    "    series=df_balanced[\"sentiment_category\"], \n",
    "    positive_label_values=[5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e8de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.bias_report(\n",
    "    df=df_balanced, \n",
    "    facet_column=facet_column, \n",
    "    label_column=label_column, \n",
    "    stage_type=report.StageType.PRE_TRAINING, \n",
    "    metrics=[\"CI\", \"DPL\", \"KL\", \"JS\", \"LP\", \"TVD\", \"KS\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bfdb39",
   "metadata": {},
   "source": [
    "#### <=== TO MIGHT NOT NEEDED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5b807",
   "metadata": {},
   "source": [
    "### Run Data Bias Analysis (Pre-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577750e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import clarify\n",
    "\n",
    "clarify_processor = clarify.SageMakerClarifyProcessor(\n",
    "    role=role, \n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.c5.xlarge\", \n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3eabcf",
   "metadata": {},
   "source": [
    "#### Pre-Training Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5defe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_report_output_path = \"s3://{}/ADS508_project/clarify\".format(bucket)\n",
    "\n",
    "bias_data_config = clarify.DataConfig(\n",
    "    s3_data_input_path=bias_data_s3_uri,\n",
    "    s3_output_path=bias_report_output_path,\n",
    "    label=\"sentiment_category\",\n",
    "    headers=df_combined.columns.to_list(),\n",
    "    dataset_type=\"text/csv\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0feb8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trump\n",
    "bias_config = clarify.BiasConfig(\n",
    "    label_values_or_threshold=[5],\n",
    "    facet_name=\"candidatepoll\",\n",
    "    facet_values_or_threshold=[\"Trump\"],\n",
    ")\n",
    "\n",
    "clarify_processor.run_pre_training_bias(\n",
    "    data_config=bias_data_config, \n",
    "    data_bias_config=bias_config, \n",
    "    methods=[\"CI\", \"DPL\", \"KL\", \"JS\", \"LP\", \"TVD\", \"KS\"],\n",
    "    wait=False, \n",
    "    logs=False\n",
    ")\n",
    "\n",
    "run_pre_training_bias_processing_job_name = clarify_processor.latest_job.job_name\n",
    "run_pre_training_bias_processing_job_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc967f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biden\n",
    "bias_config = clarify.BiasConfig(\n",
    "    label_values_or_threshold=[5],\n",
    "    facet_name=\"candidatepoll\",\n",
    "    facet_values_or_threshold=[\"Biden\"],\n",
    ")\n",
    "\n",
    "clarify_processor.run_pre_training_bias(\n",
    "    data_config=bias_data_config, \n",
    "    data_bias_config=bias_config, \n",
    "    methods=[\"CI\", \"DPL\", \"KL\", \"JS\", \"LP\", \"TVD\", \"KS\"],\n",
    "    wait=False, \n",
    "    logs=False\n",
    ")\n",
    "\n",
    "run_pre_training_bias_processing_job_name = clarify_processor.latest_job.job_name\n",
    "run_pre_training_bias_processing_job_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f6ec4",
   "metadata": {},
   "source": [
    "#### Download report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c6782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/processing-jobs/{}\">Processing Job</a></b>'.format(\n",
    "            region, run_pre_training_bias_processing_job_name\n",
    "        )\n",
    "    )\n",
    ")\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(\n",
    "            region, run_pre_training_bias_processing_job_name\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Processing Job Has Completed</b>'.format(\n",
    "            bucket, run_pre_training_bias_processing_job_name, region\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c74ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(\n",
    "    processing_job_name=run_pre_training_bias_processing_job_name, sagemaker_session=sess\n",
    ")\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee8468",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $bias_report_output_path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a98ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $bias_report_output_path ./generated_bias_report/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9329ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"./generated_bias_report/report.html\">Bias Report</a></b>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00daf0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f7808b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"./data-clarify/amazon_reviews_us_giftcards_software_videogames.csv\"\n",
    "df.to_csv(path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ba05b3-7582-40a0-8ab1-75739442b010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_testing"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "3.8.13"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "3.10.13"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
